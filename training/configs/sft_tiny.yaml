# Tunix SFT - Tiny Configuration
# Purpose: Minimal config for testing the training pipeline
# Use Case: Smoke tests, local development, CI validation

# Model Configuration
model:
  # For smoke testing, use distilgpt2 (public, no auth required)
  # For actual training, use submission_gemma_flax.yaml with google/gemma-2b-it-flax
  name: "distilgpt2"

  # Tokenizer settings
  max_length: 512  # Maximum sequence length
  padding: "max_length"
  truncation: true

# Training Configuration
training:
  # Very small number of steps for testing
  num_steps: 50

  # Learning rate
  learning_rate: 1.0e-5

  # Batch configuration
  per_device_batch_size: 1
  gradient_accumulation_steps: 1

  # Optimizer
  optimizer: "adamw"
  weight_decay: 0.01

  # Learning rate schedule
  lr_scheduler: "linear"
  warmup_steps: 5

  # Logging
  logging_steps: 10
  save_steps: 50

  # Random seed for reproducibility
  seed: 42

# System Instruction (embedded in user turn for Gemma IT)
system_instruction: "You are a helpful assistant. Show your reasoning step by step."

# Data Configuration
data:
  # Training data format
  format: "tunix_sft"

  # Shuffle
  shuffle: true
  shuffle_seed: 42

# Output Configuration
output:
  # Where to save checkpoints (relative to artifacts/training_runs/<run_id>/)
  checkpoint_dir: "checkpoint-final"

  # Whether to save metrics
  save_metrics: true
  metrics_file: "metrics.jsonl"

  # Whether to save run manifest
  save_manifest: true
  manifest_file: "run_manifest.json"

# Notes:
# - This config is intentionally minimal for smoke testing
# - Uses distilgpt2 (public, no HF auth required) for CI/testing
# - For real training, use submission_gemma_flax.yaml with google/gemma-2b-it-flax
# - For GPU training, adjust per_device_batch_size based on available VRAM
