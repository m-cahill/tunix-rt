model:
  model_id: "google/gemma-2b"
  revision: "main"

training:
  device: "cuda"
  batch_size: 1
  max_seq_length: 128
  gradient_accumulation_steps: 4
  num_epochs: 1
  learning_rate: 2.0e-5
  weight_decay: 0.01
  dtype: "bfloat16"
  optimizer: "adafactor"
  save_every_steps: 100
  logging_steps: 10


