Outstanding work. M46 is not just â€œsuccessfulâ€ â€” it **locks the Phase-5 thesis**:

> *Reasoning behavior is a trainable, observable interface â€” not a black box.*

You now have:

* Curriculum â†’ **changes reasoning structure** (M45)
* Structure â†’ **induces verification behavior** (M46)
* Metrics â†’ **separate signal from hallucination**

Thatâ€™s the exact point where M47 becomes meaningful instead of speculative.

Below is a **Cursor-ready, copy-paste prompt** for **M47**, written to be tight, falsifiable, and low-risk.

---

# ğŸ¯ Cursor Handoff Prompt â€” **M47: Injected Errors & Correction Fidelity**

**Role:**
You are an **implementation agent** working inside the existing Tunix RT / RediAI repository.

**Context (Authoritative):**

* Phase 4 is closed.
* M42 remains the immutable submission artifact.
* M45 showed curriculum reshapes reasoning.
* M46 showed explicit structure induces verification behavior (97% rate).
* **M47 tests whether that verification behavior is *meaningful***.

This milestone asks a single question:

> **When the model is wrong, can it notice â€” and fix itself?**

---

## ğŸ”’ Hard Guardrails (Non-Negotiable)

1. **DO NOT**

   * Change model architecture, optimizer, LR, batch size, tokenizer
   * Introduce RL, reward shaping, or inference-time tricks
   * Modify M42, M45, or M46 artifacts
   * Alter decoding parameters
   * Add new base datasets

2. **ALLOWED**

   * Deterministic modification of *existing* traces
   * Controlled error injection
   * New research-only scripts, metrics, and analysis

3. **All changes must be additive, reversible, and documented.**

---

## ğŸ§­ Objective (M47)

Test the hypothesis:

> **Verification behavior learned in M46 can detect and correct real errors â€” not just perform ritualized checking.**

This is a **fidelity test**, not an accuracy benchmark.

---

## ğŸ§  Core Concept â€” Controlled Error Injection

You will introduce **known, localized errors** into a **small, well-defined subset** of reasoning traces and measure whether the model:

1. Detects the error in `VERIFY:`
2. Produces a meaningful `CORRECT:`
3. Improves (or fails to improve) the final answer

---

## ğŸ§± Scope & Dataset Design

### Source Dataset (Fixed)

* Base: `stage_c.jsonl` from M45
* Size: 341 samples

### Injection Rate (LOCKED)

* **10% of samples (~34 traces)**
* Chosen deterministically (seeded)
* Error locations must be explicitly logged

---

## ğŸ”§ Error Types (Choose 2â€“3 Only)

You must select **at most three** error classes.

Recommended set (balanced, interpretable):

1. **Arithmetic Slip**

   * Off-by-one
   * Sign error
   * Simple miscalculation

2. **Unit / Conversion Error**

   * Wrong unit cancellation
   * Incorrect scale factor

3. **Logic Step Omission**

   * Skipped intermediate step
   * Incorrect assumption carried forward

**Do NOT** inject:

* Random nonsense
* Multiple errors per trace
* Errors in the final answer *only* (error must be in reasoning)

---

## ğŸ—‚ï¸ Dataset Variants (Three-Way Comparison)

Create **three datasets**:

1. **Clean Self-Correct**

   * Same as M46 self-correct dataset
   * No errors

2. **Error-Injected (Unlabeled)**

   * Errors injected
   * No indication to the model that an error exists

3. **Error-Injected + Self-Correct Structure**

   * Same injected errors
   * With `VERIFY:` / `CORRECT:` structure

---

## ğŸ—ï¸ Implementation Tasks (Execute in Order)

### 1. Error Injection Script

Create a deterministic script that:

* Selects ~10% of traces
* Injects exactly one error per trace
* Logs:

  * Trace ID
  * Error type
  * Location (step index)
  * Ground-truth correction

Output:

* `stage_c_clean.jsonl`
* `stage_c_error.jsonl`
* `stage_c_error_self_correct.jsonl`
* `error_manifest.json`

---

### 2. Training Runs (Minimal, Controlled)

Run **two training jobs**:

| Run         | Dataset                    | Epochs | Init                             |
| ----------- | -------------------------- | ------ | -------------------------------- |
| Clean       | stage_c_clean              | 1      | M46 Self-Correct checkpoint      |
| Error-Aware | stage_c_error_self_correct | 1      | Same M46 Self-Correct checkpoint |

No third run needed â€” the unlabeled error dataset is for evaluation only.

---

### 3. Evaluation Matrix

Evaluate **all three checkpoints** on:

* Clean eval set
* Error-injected eval set

Track outputs for:

* Detection
* Correction
* Final answer

---

## ğŸ“Š Core Metrics (Must Be Computed)

At minimum:

1. **Error Detection Rate**

   * Error mentioned or contradicted in `VERIFY:`

2. **Correction Attempt Rate**

   * `CORRECT:` block present and non-empty

3. **Correction Accuracy**

   * Correction fixes the injected error

4. **False Correction Rate**

   * Model â€œfixesâ€ something that wasnâ€™t wrong

5. **Net Outcome**

   * Final answer improved / unchanged / worse

These must be **machine-counted**, not anecdotal.

---

## ğŸ§ª Qualitative Analysis (Required)

Include:

* 2â€“3 **successful self-corrections**
* 2â€“3 **failure modes**, such as:

  * Detects error but fails to correct
  * Hallucinates wrong correction
  * Performs ritual verification only

Label failures clearly.
Negative results are valid.

---

## ğŸ“¦ Deliverables Checklist

* [ ] Error injection script
* [ ] Three dataset variants
* [ ] Two fine-tuned checkpoints
* [ ] Evaluation outputs
* [ ] Metrics summary (JSON + table)
* [ ] `analysis.md`
* [ ] `error_manifest.json`
* [ ] Updated provenance manifest

---

## ğŸ›‘ Stop Conditions

Immediately stop and report if:

* False corrections exceed **25%**
* Corrections systematically worsen answers
* Model collapses into repetitive correction loops

Do **not** â€œpatch forward.â€

---

## ğŸ“ Directory Structure (LOCKED)

```
research/
  m47_error_correction_fidelity/
    data/
    scripts/
    checkpoints/
    eval/
    metrics/
    analysis.md
    error_manifest.json
    provenance.json
```

No files elsewhere.

---

## ğŸ§  Framing Reminder

This milestone does **not** claim:

âŒ â€œThe model is reliableâ€
âŒ â€œThe model reasons correctlyâ€

It **does** demonstrate:

âœ… Verification can become **error-sensitive**
âœ… Corrections can be **measured**
âœ… Failure modes can be **cataloged**

This sets up M48/M49 cleanly.

---

## ğŸ”œ What M47 Unlocks

If M47 shows *any* non-trivial correction fidelity:

* M48: **Failure topology** (why it fails)
* M49: **Observer / judge models**
* M50: **System self-audit narrative**

You are no longer training models.

You are **instrumenting reasoning**.

---

When youâ€™re ready, hand this prompt to Cursor.

If you want next:

* I can pre-define **metric thresholds** (what counts as â€œmeaningfulâ€)
* Or help you decide **which error types to prioritize** for maximum signal

But as written: **M47 is fully specified and ready to execute.**
