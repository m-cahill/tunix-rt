{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tunix RT - Kaggle Submission Notebook\n",
        "\n",
        "**Competition:** Google Tunix Hack - Train a model to show its work  \n",
        "**Version:** `m36_v4`\n",
        "\n",
        "This notebook provides a single-session workflow for the Tunix Hack competition.\n",
        "\n",
        "**Workflow:**\n",
        "1. **Clone repository** (required on Kaggle)\n",
        "2. Install dependencies\n",
        "3. Configure training parameters\n",
        "4. Build/load dataset\n",
        "5. Train model (JAX/Flax) - Smoke or Full mode\n",
        "6. Generate predictions\n",
        "7. Evaluate and score (eval_v2: 100 items with scorecard)\n",
        "8. Display submission summary with RESULT SUMMARY block\n",
        "\n",
        "**Runtime:** Kaggle TPU or GPU (recommended)  \n",
        "**Time:** ~5 min (smoke) / ~1-2 hours (full)\n",
        "\n",
        "**‚ö†Ô∏è Important:** Run the \"Clone Repository\" cell first before any other cells!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Clone Repository (Required on Kaggle)\n",
        "\n",
        "**Run this cell first!** It clones the tunix-rt repository so all training scripts and tools are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the tunix-rt repository\n",
        "# This provides all training scripts, tools, and configurations\n",
        "# Uses absolute paths to prevent nested directory issues on re-run\n",
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/m-cahill/tunix-rt.git\"\n",
        "KAGGLE_WORKING = \"/kaggle/working\"\n",
        "REPO_DIR = f\"{KAGGLE_WORKING}/tunix-rt\"  # Absolute path\n",
        "\n",
        "# Check if already cloned (for re-running cells)\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"üìÅ Repository already exists at {REPO_DIR}\")\n",
        "else:\n",
        "    print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
        "    os.chdir(KAGGLE_WORKING)\n",
        "    !git clone {REPO_URL}\n",
        "    print(f\"‚úÖ Repository cloned successfully!\")\n",
        "\n",
        "# Always cd to the repo directory (idempotent - safe to re-run)\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Verify we're in the right directory\n",
        "print(f\"\\nüìç Working directory: {os.getcwd()}\")\n",
        "print(f\"üìÇ Contents: {os.listdir('.')[:10]}...\")  # Show first 10 items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Install dependencies and verify JAX is working.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (Kaggle environment)\n",
        "# Note: JAX with TPU support is pre-installed on Kaggle TPU runtimes\n",
        "!pip install -q jax[cuda12] flax optax orbax-checkpoint transformers datasets pyyaml\n",
        "\n",
        "# Verify JAX installation\n",
        "import jax\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Available devices: {jax.devices()}\")\n",
        "print(\"\\n‚úÖ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Configure training parameters below. The notebook supports two modes:\n",
        "- **Smoke Mode:** Quick validation (2 steps, ~5 min)\n",
        "- **Full Mode:** Complete training run (~1-2 hours)\n",
        "\n",
        "**Note:** Paths are relative to the cloned repository root.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION - Modify these values as needed\n",
        "# ============================================================\n",
        "\n",
        "# Config file selection (model name is inside the config)\n",
        "# Options:\n",
        "#   - training/configs/submission_gemma3_1b.yaml (Gemma 3 1B-it, recommended)\n",
        "#   - training/configs/submission_gemma2_2b.yaml (Gemma 2 2B)\n",
        "#   - training/configs/sft_tiny.yaml (for testing only)\n",
        "CONFIG_PATH = \"training/configs/submission_gemma3_1b.yaml\"\n",
        "\n",
        "# Dataset selection\n",
        "# Options: dev-reasoning-v2 (550 traces, recommended), golden-v2 (100 traces, quick sanity)\n",
        "DATASET = \"dev-reasoning-v2\"\n",
        "\n",
        "# Training parameters\n",
        "SMOKE_STEPS = 2        # Smoke run: 2 steps for validation\n",
        "\n",
        "# Device selection\n",
        "DEVICE = \"auto\"  # auto-detect GPU/TPU, or \"cpu\" for testing\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = \"./output/kaggle_run\"\n",
        "SMOKE_OUTPUT_DIR = \"./output/smoke_run\"\n",
        "\n",
        "# Evaluation (M36: eval_v2 with 100 items and scorecard support)\n",
        "# Options: eval_v2.jsonl (100 items, recommended), eval_v1.jsonl (50 items, legacy)\n",
        "EVAL_SET = \"training/evalsets/eval_v2.jsonl\"\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# Load config to display model name\n",
        "import yaml\n",
        "with open(CONFIG_PATH) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "MODEL_NAME = config.get('model', {}).get('name', 'unknown')\n",
        "MAX_STEPS = config.get('training', {}).get('num_steps', 100)\n",
        "SEED = config.get('training', {}).get('seed', 42)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Config:      {CONFIG_PATH}\")\n",
        "print(f\"  Model:       {MODEL_NAME}\")\n",
        "print(f\"  Dataset:     {DATASET}\")\n",
        "print(f\"  Max Steps:   {MAX_STEPS} (from config)\")\n",
        "print(f\"  Smoke Steps: {SMOKE_STEPS}\")\n",
        "print(f\"  Seed:        {SEED}\")\n",
        "print(f\"  Device:      {DEVICE}\")\n",
        "print(f\"  Eval Set:    {EVAL_SET}\")\n",
        "print(f\"  Output:      {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Dataset\n",
        "\n",
        "Seed scripts are located in `backend/tools/` and write to `backend/datasets/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the selected dataset\n",
        "# Note: Datasets are deterministically seeded (seed=42)\n",
        "\n",
        "if DATASET == \"dev-reasoning-v2\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_dev_reasoning_v2.py\"])\n",
        "elif DATASET == \"golden-v2\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_golden_v2.py\"])\n",
        "elif DATASET == \"dev-reasoning-v1\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_dev_reasoning_v1.py\"])\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Dataset {DATASET} not recognized, assuming it already exists\")\n",
        "\n",
        "# Verify dataset exists\n",
        "dataset_path = Path(f\"backend/datasets/{DATASET}\")\n",
        "if dataset_path.exists():\n",
        "    manifest_path = dataset_path / \"manifest.json\"\n",
        "    if manifest_path.exists():\n",
        "        with open(manifest_path) as f:\n",
        "            manifest = json.load(f)\n",
        "        print(f\"\\n‚úÖ Dataset ready: {DATASET}\")\n",
        "        print(f\"   Traces: {manifest.get('trace_count', 'N/A')}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Manifest not found at {manifest_path}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Dataset directory not found: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4a. Smoke Run (Quick Validation)\n",
        "\n",
        "Run this cell first to validate the pipeline works before the full training run.\n",
        "\n",
        "**Recommended:** Always run smoke first to verify environment before committing to full training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMOKE RUN - Quick validation (2 steps)\n",
        "# This confirms imports, dataset loading, and basic training work correctly\n",
        "\n",
        "print(\"üî• Starting Smoke Run (2 steps)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "smoke_cmd = [\n",
        "    sys.executable, \"training/train_jax.py\",\n",
        "    \"--config\", CONFIG_PATH,\n",
        "    \"--output\", SMOKE_OUTPUT_DIR,\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--device\", DEVICE,\n",
        "    \"--smoke_steps\", str(SMOKE_STEPS),\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(smoke_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(smoke_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Smoke run completed successfully!\")\n",
        "    print(\"   Pipeline validated. Ready for full training.\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Smoke run failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4b. Full Training Run\n",
        "\n",
        "Run this cell for the complete training.\n",
        "\n",
        "**Time budget:** ~1-2 hours for 100 steps on TPU/GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FULL TRAINING RUN\n",
        "# This runs the complete training pipeline with the configured parameters\n",
        "\n",
        "print(\"üöÄ Starting Full Training Run...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Config:    {CONFIG_PATH}\")\n",
        "print(f\"Model:     {MODEL_NAME}\")\n",
        "print(f\"Dataset:   {DATASET}\")\n",
        "print(f\"Steps:     {MAX_STEPS} (from config)\")\n",
        "print(f\"Output:    {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "train_cmd = [\n",
        "    sys.executable, \"training/train_jax.py\",\n",
        "    \"--config\", CONFIG_PATH,\n",
        "    \"--output\", OUTPUT_DIR,\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--device\", DEVICE,\n",
        "    \"--save_every_steps\", \"50\",\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(train_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(train_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Training failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Predictions\n",
        "\n",
        "Generate predictions on the evaluation set using the trained checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the evaluation set\n",
        "\n",
        "predictions_file = f\"{OUTPUT_DIR}/predictions.jsonl\"\n",
        "\n",
        "print(\"üìä Generating predictions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_cmd = [\n",
        "    sys.executable, \"training/eval_generate.py\",\n",
        "    \"--checkpoint\", OUTPUT_DIR,\n",
        "    \"--eval_set\", EVAL_SET,\n",
        "    \"--output\", predictions_file,\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(eval_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(eval_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úÖ Predictions generated successfully!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Prediction generation failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate & Score\n",
        "\n",
        "Score predictions using the evaluation script with scorecard breakdown.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score predictions using the evaluation script\n",
        "\n",
        "print(\"üìà Scoring predictions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "score_cmd = [\n",
        "    sys.executable, \"training/eval_report.py\",\n",
        "    \"--predictions\", predictions_file,\n",
        "    \"--eval_set\", EVAL_SET,\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(score_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(score_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úÖ Evaluation complete!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Evaluation failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Submission Summary\n",
        "\n",
        "Displays final results with a **RESULT SUMMARY** block for easy evidence capture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final submission summary with RESULT SUMMARY block for evidence capture\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"         SUBMISSION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "output_path = Path(OUTPUT_DIR)\n",
        "\n",
        "# Model info\n",
        "print(f\"\\nüì¶ Model ID: {MODEL_NAME}\")\n",
        "print(f\"üìÅ Dataset:  {DATASET}\")\n",
        "print(f\"üìã Eval Set: {EVAL_SET}\")\n",
        "print(f\"üî¢ Steps:    {MAX_STEPS}\")\n",
        "print(f\"üé≤ Seed:     {SEED}\")\n",
        "\n",
        "# Training metrics\n",
        "metrics_file = output_path / \"metrics.jsonl\"\n",
        "final_loss = None\n",
        "if metrics_file.exists():\n",
        "    print(f\"\\nüìä Training Metrics (last 5 steps):\")\n",
        "    with open(metrics_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-5:]:\n",
        "            metric = json.loads(line)\n",
        "            step = metric.get('step', '?')\n",
        "            loss = metric.get('loss', '?')\n",
        "            if isinstance(loss, float):\n",
        "                print(f\"   Step {step}: loss={loss:.4f}\")\n",
        "                final_loss = loss\n",
        "            else:\n",
        "                print(f\"   Step {step}: loss={loss}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Metrics file not found at {metrics_file}\")\n",
        "\n",
        "# Eval score and scorecard\n",
        "eval_results_file = output_path / \"eval_results.json\"\n",
        "primary_score = None\n",
        "scorecard_info = {}\n",
        "if eval_results_file.exists():\n",
        "    with open(eval_results_file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "        primary_score = results.get('primary_score', results.get('answer_correctness'))\n",
        "        scorecard_info = results.get('scorecard', {})\n",
        "        if isinstance(primary_score, float):\n",
        "            print(f\"\\nüéØ Primary Score: {primary_score:.4f} ({primary_score * 100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"\\nüéØ Primary Score: {primary_score}\")\n",
        "        \n",
        "        # M36: Display scorecard if available\n",
        "        if scorecard_info:\n",
        "            n_items = scorecard_info.get('n_items', '?')\n",
        "            n_scored = scorecard_info.get('n_scored', '?')\n",
        "            print(f\"üìä Scorecard: {n_scored}/{n_items} items scored\")\n",
        "            section_scores = scorecard_info.get('section_scores', {})\n",
        "            for section, score in section_scores.items():\n",
        "                if score is not None:\n",
        "                    print(f\"   {section}: {score:.2f}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Eval results not found (run evaluation cell first)\")\n",
        "\n",
        "# Artifact paths\n",
        "print(f\"\\nüìÇ Artifact Paths:\")\n",
        "if output_path.exists():\n",
        "    checkpoints = list(output_path.glob(\"checkpoint*\"))\n",
        "    for ckpt in checkpoints:\n",
        "        print(f\"   {ckpt}\")\n",
        "if metrics_file.exists():\n",
        "    print(f\"   {metrics_file}\")\n",
        "preds_path = Path(predictions_file)\n",
        "if preds_path.exists():\n",
        "    print(f\"   {preds_path}\")\n",
        "\n",
        "# M36: Print RESULT SUMMARY block for evidence capture\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"         RESULT SUMMARY (copy to evidence files)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"model_id: {MODEL_NAME}\")\n",
        "print(f\"dataset: {DATASET}\")\n",
        "print(f\"eval_set: {EVAL_SET}\")\n",
        "print(f\"primary_score: {primary_score}\")\n",
        "print(f\"final_loss: {final_loss}\")\n",
        "print(f\"n_items: {scorecard_info.get('n_items', 'N/A')}\")\n",
        "print(f\"n_scored: {scorecard_info.get('n_scored', 'N/A')}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n‚úÖ Submission package ready!\")\n",
        "print(\"   See docs/submission_checklist.md for next steps.\")\n",
        "print(\"   See docs/M36_KAGGLE_RUN.md for evidence capture instructions.\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
