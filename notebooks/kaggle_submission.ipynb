{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tunix RT - Kaggle Submission Notebook\n",
        "\n",
        "**Competition:** Google Tunix Hack - Train a model to show its work  \n",
        "**Version:** `m33_v1`\n",
        "\n",
        "This notebook provides a single-session workflow for the Tunix Hack competition.\n",
        "\n",
        "**Workflow:**\n",
        "1. Install dependencies\n",
        "2. Configure training parameters\n",
        "3. Build/load dataset\n",
        "4. Train model (JAX/Flax) - Smoke or Full mode\n",
        "5. Generate predictions\n",
        "6. Evaluate and score\n",
        "7. Display submission summary\n",
        "\n",
        "**Runtime:** Kaggle TPU or GPU (recommended)  \n",
        "**Time:** ~5 min (smoke) / ~1-2 hours (full)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (Kaggle environment)\n",
        "# Note: JAX with TPU support is pre-installed on Kaggle TPU runtimes\n",
        "!pip install -q jax[cuda12] flax optax orbax-checkpoint transformers datasets\n",
        "\n",
        "# Verify JAX installation\n",
        "import jax\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Available devices: {jax.devices()}\")\n",
        "print(\"\\n‚úÖ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Configure training parameters below. The notebook supports two modes:\n",
        "- **Smoke Mode:** Quick validation (2 steps, ~5 min)\n",
        "- **Full Mode:** Complete training run (~1-2 hours)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION - Modify these values as needed\n",
        "# ============================================================\n",
        "\n",
        "# Model selection (competition requirement: Gemma 2 2B or Gemma 3 1B)\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"  # or \"google/gemma-2-2b\"\n",
        "\n",
        "# Dataset selection\n",
        "# Options: dev-reasoning-v2 (550 traces, recommended), golden-v2 (100 traces, quick sanity)\n",
        "DATASET = \"dev-reasoning-v2\"\n",
        "\n",
        "# Training parameters\n",
        "MAX_STEPS = 100        # Full run: 100-1000 steps\n",
        "SMOKE_STEPS = 2        # Smoke run: 2 steps for validation\n",
        "SEED = 42\n",
        "\n",
        "# Device selection\n",
        "DEVICE = \"auto\"  # auto-detect GPU/TPU, or \"cpu\" for testing\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = \"./output/kaggle_run\"\n",
        "SMOKE_OUTPUT_DIR = \"./output/smoke_run\"\n",
        "\n",
        "# Evaluation\n",
        "EVAL_SET = \"training/evalsets/eval_v1.jsonl\"\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model:       {MODEL_NAME}\")\n",
        "print(f\"  Dataset:     {DATASET}\")\n",
        "print(f\"  Max Steps:   {MAX_STEPS}\")\n",
        "print(f\"  Smoke Steps: {SMOKE_STEPS}\")\n",
        "print(f\"  Seed:        {SEED}\")\n",
        "print(f\"  Device:      {DEVICE}\")\n",
        "print(f\"  Output:      {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the selected dataset\n",
        "# Note: Datasets are deterministically seeded (seed=42)\n",
        "\n",
        "if DATASET == \"dev-reasoning-v2\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_dev_reasoning_v2.py\"])\n",
        "elif DATASET == \"golden-v2\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_golden_v2.py\"])\n",
        "elif DATASET == \"dev-reasoning-v1\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_dev_reasoning_v1.py\"])\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Dataset {DATASET} not recognized, assuming it already exists\")\n",
        "\n",
        "# Verify dataset exists\n",
        "dataset_path = Path(f\"backend/datasets/{DATASET}\")\n",
        "if dataset_path.exists():\n",
        "    manifest_path = dataset_path / \"manifest.json\"\n",
        "    if manifest_path.exists():\n",
        "        with open(manifest_path) as f:\n",
        "            manifest = json.load(f)\n",
        "        print(f\"\\n‚úÖ Dataset ready: {DATASET}\")\n",
        "        print(f\"   Traces: {manifest.get('trace_count', 'N/A')}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Manifest not found at {manifest_path}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Dataset directory not found: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4a. Smoke Run (Quick Validation)\n",
        "\n",
        "Run this cell first to validate the pipeline works before the full training run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMOKE RUN - Quick validation (2 steps)\n",
        "# This confirms imports, dataset loading, and basic training work correctly\n",
        "\n",
        "print(\"üî• Starting Smoke Run (2 steps)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "smoke_cmd = [\n",
        "    sys.executable, \"training/train_jax.py\",\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--model_name\", MODEL_NAME,\n",
        "    \"--device\", DEVICE,\n",
        "    \"--output_dir\", SMOKE_OUTPUT_DIR,\n",
        "    \"--seed\", str(SEED),\n",
        "    \"--smoke_steps\", str(SMOKE_STEPS),\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(smoke_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(smoke_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Smoke run completed successfully!\")\n",
        "    print(\"   Pipeline validated. Ready for full training.\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Smoke run failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4b. Full Training Run\n",
        "\n",
        "Run this cell for the complete training. Time budget: ~1-2 hours for 100 steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FULL TRAINING RUN\n",
        "# This runs the complete training pipeline with the configured parameters\n",
        "\n",
        "print(\"üöÄ Starting Full Training Run...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model:     {MODEL_NAME}\")\n",
        "print(f\"Dataset:   {DATASET}\")\n",
        "print(f\"Steps:     {MAX_STEPS}\")\n",
        "print(f\"Output:    {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "train_cmd = [\n",
        "    sys.executable, \"training/train_jax.py\",\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--model_name\", MODEL_NAME,\n",
        "    \"--max_steps\", str(MAX_STEPS),\n",
        "    \"--device\", DEVICE,\n",
        "    \"--output_dir\", OUTPUT_DIR,\n",
        "    \"--seed\", str(SEED),\n",
        "    \"--save_every_steps\", \"50\",\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(train_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(train_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Training failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the evaluation set\n",
        "\n",
        "predictions_file = f\"{OUTPUT_DIR}/predictions.jsonl\"\n",
        "\n",
        "print(\"üìä Generating predictions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_cmd = [\n",
        "    sys.executable, \"training/eval_generate.py\",\n",
        "    \"--checkpoint\", OUTPUT_DIR,\n",
        "    \"--eval_set\", EVAL_SET,\n",
        "    \"--output\", predictions_file,\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(eval_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(eval_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úÖ Predictions generated successfully!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Prediction generation failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate & Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score predictions using the evaluation script\n",
        "\n",
        "print(\"üìà Scoring predictions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "score_cmd = [\n",
        "    sys.executable, \"training/eval_report.py\",\n",
        "    \"--predictions\", predictions_file,\n",
        "    \"--eval_set\", EVAL_SET,\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(score_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(score_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úÖ Evaluation complete!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Evaluation failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Submission Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final submission summary\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"         SUBMISSION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "output_path = Path(OUTPUT_DIR)\n",
        "\n",
        "# Model info\n",
        "print(f\"\\nüì¶ Model ID: {MODEL_NAME}\")\n",
        "print(f\"üìÅ Dataset:  {DATASET}\")\n",
        "print(f\"üî¢ Steps:    {MAX_STEPS}\")\n",
        "print(f\"üé≤ Seed:     {SEED}\")\n",
        "\n",
        "# Training metrics\n",
        "metrics_file = output_path / \"metrics.jsonl\"\n",
        "if metrics_file.exists():\n",
        "    print(f\"\\nüìä Training Metrics (last 5 steps):\")\n",
        "    with open(metrics_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-5:]:\n",
        "            metric = json.loads(line)\n",
        "            step = metric.get('step', '?')\n",
        "            loss = metric.get('loss', '?')\n",
        "            if isinstance(loss, float):\n",
        "                print(f\"   Step {step}: loss={loss:.4f}\")\n",
        "            else:\n",
        "                print(f\"   Step {step}: loss={loss}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Metrics file not found at {metrics_file}\")\n",
        "\n",
        "# Eval score\n",
        "eval_results_file = output_path / \"eval_results.json\"\n",
        "if eval_results_file.exists():\n",
        "    with open(eval_results_file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "        score = results.get('answer_correctness', 'N/A')\n",
        "        if isinstance(score, float):\n",
        "            print(f\"\\nüéØ Eval Score: {score:.2f}\")\n",
        "        else:\n",
        "            print(f\"\\nüéØ Eval Score: {score}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Eval results not found (run evaluation cell first)\")\n",
        "\n",
        "# Artifact paths\n",
        "print(f\"\\nüìÇ Artifact Paths:\")\n",
        "if output_path.exists():\n",
        "    checkpoints = list(output_path.glob(\"checkpoint*\"))\n",
        "    for ckpt in checkpoints:\n",
        "        print(f\"   {ckpt}\")\n",
        "if metrics_file.exists():\n",
        "    print(f\"   {metrics_file}\")\n",
        "preds_path = Path(predictions_file)\n",
        "if preds_path.exists():\n",
        "    print(f\"   {preds_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Submission package ready!\")\n",
        "print(\"   See docs/submission_checklist.md for next steps.\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
