{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tunix RT - Kaggle Submission Notebook\n",
        "\n",
        "**Competition:** Google Tunix Hack - Train a model to show its work  \n",
        "**Version:** `m38_v1` ‚Äî TPU Training with HBM OOM Fix\n",
        "\n",
        "This notebook provides a single-session workflow for the Tunix Hack competition.\n",
        "\n",
        "**Workflow:**\n",
        "1. **Clone repository** (required on Kaggle)\n",
        "2. Install dependencies\n",
        "3. **Authenticate with HuggingFace** (required for gated Gemma models)\n",
        "4. Configure training parameters\n",
        "5. Build/load dataset\n",
        "6. **Smoke test** (tiny model, validates pipeline on GPU)\n",
        "7. **Full TPU training** (Gemma 2B, **requires TPU v3-8 or v5e-8**)\n",
        "8. Generate predictions\n",
        "9. Evaluate and score (eval_v2: 100 items with scorecard)\n",
        "10. Display submission summary with RESULT SUMMARY block\n",
        "\n",
        "**Runtime:**\n",
        "- **Smoke test:** Any GPU (uses tiny model)\n",
        "- **Full training:** **TPU v3-8 or v5e-8 REQUIRED** (Gemma 2B will NOT fit on GPU)\n",
        "\n",
        "**Time:** ~1 min (smoke) / ~30-60 min (200 steps on TPU)\n",
        "\n",
        "**‚ö†Ô∏è Important (M38):**\n",
        "- Run the \"Clone Repository\" cell first before any other cells!\n",
        "- For full training, you **MUST** switch to **TPU** accelerator (Settings ‚Üí Accelerator ‚Üí TPU v3-8)\n",
        "- GPU training with Gemma 2B is **blocked** by the training script (will exit with error)\n",
        "- **M38 Fix:** Uses `%run` instead of subprocess to avoid TPU VFIO conflicts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Clone Repository (Required on Kaggle)\n",
        "\n",
        "**Run this cell first!** It clones the tunix-rt repository so all training scripts and tools are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the tunix-rt repository\n",
        "# This provides all training scripts, tools, and configurations\n",
        "# Uses absolute paths to prevent nested directory issues on re-run\n",
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/m-cahill/tunix-rt.git\"\n",
        "KAGGLE_WORKING = \"/kaggle/working\"\n",
        "REPO_DIR = f\"{KAGGLE_WORKING}/tunix-rt\"  # Absolute path\n",
        "\n",
        "# Check if already cloned (for re-running cells)\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"üìÅ Repository already exists at {REPO_DIR}\")\n",
        "else:\n",
        "    print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
        "    os.chdir(KAGGLE_WORKING)\n",
        "    !git clone {REPO_URL}\n",
        "    print(f\"‚úÖ Repository cloned successfully!\")\n",
        "\n",
        "# Always cd to the repo directory (idempotent - safe to re-run)\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Verify we're in the right directory\n",
        "print(f\"\\nüìç Working directory: {os.getcwd()}\")\n",
        "print(f\"üìÇ Contents: {os.listdir('.')[:10]}...\")  # Show first 10 items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Install dependencies and verify JAX is working.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (Kaggle environment)\n",
        "# Note: JAX with TPU support is pre-installed on Kaggle TPU runtimes.\n",
        "#       For GPU, we install jax[cuda12]. Pin transformers<5 for Flax support.\n",
        "\n",
        "# Detect if TPU is available (Kaggle TPU has JAX pre-installed)\n",
        "import subprocess\n",
        "result = subprocess.run([\"pip\", \"show\", \"jax\"], capture_output=True, text=True)\n",
        "has_jax = result.returncode == 0\n",
        "\n",
        "if not has_jax:\n",
        "    print(\"Installing JAX with CUDA support...\")\n",
        "    !pip install -q \"jax[cuda12]\"\n",
        "\n",
        "# Install other dependencies (transformers v4 for Flax support)\n",
        "!pip install -q flax optax orbax-checkpoint \"transformers>=4.40,<5\" datasets pyyaml huggingface_hub\n",
        "\n",
        "# Verify JAX installation\n",
        "import jax\n",
        "print(f\"\\nJAX version: {jax.__version__}\")\n",
        "print(f\"Available devices: {jax.devices()}\")\n",
        "print(f\"Default backend: {jax.default_backend()}\")\n",
        "\n",
        "# Warn about device type\n",
        "backend = jax.default_backend()\n",
        "if backend == \"gpu\":\n",
        "    print(\"\\n‚ö†Ô∏è  Running on GPU. Smoke tests will work, but full Gemma training may OOM.\")\n",
        "    print(\"   For full training, switch to TPU (Settings ‚Üí Accelerator ‚Üí TPU v3-8)\")\n",
        "elif backend == \"tpu\":\n",
        "    print(\"\\n‚úÖ Running on TPU. Full Gemma training is supported.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Running on {backend}. Performance may be limited.\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 HuggingFace Authentication (Required)\n",
        "\n",
        "**Gemma models are gated.** You must authenticate with HuggingFace before loading.\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Accept the Gemma license at https://huggingface.co/google/gemma-2b-flax\n",
        "2. Add your HuggingFace token as a Kaggle Secret named `HF_TOKEN`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with HuggingFace (required for gated Gemma models)\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "from huggingface_hub import login\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# Verify authentication\n",
        "from huggingface_hub import whoami\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"‚úÖ Logged in to HuggingFace as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå HuggingFace authentication failed: {e}\")\n",
        "    print(\"   Make sure HF_TOKEN secret is set in Kaggle Secrets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Configure training parameters below. The notebook supports two modes:\n",
        "- **Smoke Mode:** Quick validation (2 steps, ~5 min)\n",
        "- **Full Mode:** Complete training run (~1-2 hours)\n",
        "\n",
        "**Note:** Paths are relative to the cloned repository root.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION - Modify these values as needed\n",
        "# ============================================================\n",
        "\n",
        "# M38: TPU Training Config (200 steps, memory-optimized for Gemma 256K vocab)\n",
        "# This is the PRIMARY config for submission training.\n",
        "# ‚ö†Ô∏è  DO NOT use this on GPU ‚Äî it will block with an error.\n",
        "# M38 fix: seq_len=128, batch=1, grad_accum=8, bfloat16 to avoid HBM OOM\n",
        "CONFIG_PATH = \"training/configs/submission_tpu.yaml\"\n",
        "\n",
        "# Config file for SMOKE TESTS (tiny model that fits on any GPU)\n",
        "# This validates the pipeline works without OOM issues\n",
        "SMOKE_CONFIG_PATH = \"training/configs/smoke_tiny.yaml\"\n",
        "\n",
        "# Dataset selection\n",
        "# Options: dev-reasoning-v2 (550 traces, recommended), golden-v2 (100 traces, quick sanity)\n",
        "DATASET = \"dev-reasoning-v2\"\n",
        "\n",
        "# Training parameters\n",
        "SMOKE_STEPS = 2        # Smoke run: 2 steps for validation\n",
        "\n",
        "# Device selection\n",
        "# M37: Use \"tpu\" for full training, \"auto\" for smoke tests\n",
        "DEVICE_SMOKE = \"auto\"  # auto-detect GPU/TPU for smoke tests\n",
        "DEVICE_FULL = \"tpu\"    # Explicit TPU for full training (M37)\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = \"./output/tpu_run\"      # M37: TPU training output\n",
        "SMOKE_OUTPUT_DIR = \"./output/smoke_run\"\n",
        "\n",
        "# Evaluation (M36+: eval_v2 with 100 items and scorecard support)\n",
        "# Options: eval_v2.jsonl (100 items, recommended), eval_v1.jsonl (50 items, legacy)\n",
        "EVAL_SET = \"training/evalsets/eval_v2.jsonl\"\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# Load configs to display model names\n",
        "import yaml\n",
        "with open(CONFIG_PATH) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "MODEL_NAME = config.get('model', {}).get('name', 'unknown')\n",
        "MAX_STEPS = config.get('training', {}).get('num_steps', 100)\n",
        "SEED = config.get('training', {}).get('seed', 42)\n",
        "\n",
        "with open(SMOKE_CONFIG_PATH) as f:\n",
        "    smoke_config = yaml.safe_load(f)\n",
        "SMOKE_MODEL_NAME = smoke_config.get('model', {}).get('name', 'unknown')\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Full Config:  {CONFIG_PATH}\")\n",
        "print(f\"  Full Model:   {MODEL_NAME}\")\n",
        "print(f\"  Smoke Config: {SMOKE_CONFIG_PATH}\")\n",
        "print(f\"  Smoke Model:  {SMOKE_MODEL_NAME}\")\n",
        "print(f\"  Dataset:      {DATASET}\")\n",
        "print(f\"  Max Steps:    {MAX_STEPS} (from full config)\")\n",
        "print(f\"  Smoke Steps:  {SMOKE_STEPS}\")\n",
        "print(f\"  Seed:         {SEED}\")\n",
        "print(f\"  Device (Smoke): {DEVICE_SMOKE}\")\n",
        "print(f\"  Device (Full):  {DEVICE_FULL}\")\n",
        "print(f\"  Eval Set:     {EVAL_SET}\")\n",
        "print(f\"  Output:       {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Dataset\n",
        "\n",
        "Seed scripts are located in `backend/tools/` and write to `backend/datasets/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the selected dataset\n",
        "# Note: Datasets are deterministically seeded (seed=42)\n",
        "\n",
        "if DATASET == \"dev-reasoning-v2\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_dev_reasoning_v2.py\"])\n",
        "elif DATASET == \"golden-v2\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_golden_v2.py\"])\n",
        "elif DATASET == \"dev-reasoning-v1\":\n",
        "    subprocess.run([sys.executable, \"backend/tools/seed_dev_reasoning_v1.py\"])\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Dataset {DATASET} not recognized, assuming it already exists\")\n",
        "\n",
        "# Verify dataset exists\n",
        "dataset_path = Path(f\"backend/datasets/{DATASET}\")\n",
        "if dataset_path.exists():\n",
        "    manifest_path = dataset_path / \"manifest.json\"\n",
        "    if manifest_path.exists():\n",
        "        with open(manifest_path) as f:\n",
        "            manifest = json.load(f)\n",
        "        print(f\"\\n‚úÖ Dataset ready: {DATASET}\")\n",
        "        print(f\"   Traces: {manifest.get('trace_count', 'N/A')}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Manifest not found at {manifest_path}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Dataset directory not found: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4a. Smoke Run (Quick Validation)\n",
        "\n",
        "Run this cell first to validate the pipeline works before the full training run.\n",
        "\n",
        "**Recommended:** Always run smoke first to verify environment before committing to full training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMOKE RUN - Quick validation (2 steps)\n",
        "# This confirms imports, dataset loading, and basic training work correctly\n",
        "#\n",
        "# NOTE: We use --smoke_config to load a tiny model (sshleifer/tiny-gpt2) that\n",
        "# fits on any GPU. This validates the PIPELINE without OOM issues.\n",
        "# The full config (CONFIG_PATH) is still passed but not used during smoke.\n",
        "\n",
        "print(\"üî• Starting Smoke Run (2 steps)...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"   Using smoke config: {SMOKE_CONFIG_PATH}\")\n",
        "print(f\"   Smoke model: {SMOKE_MODEL_NAME}\")\n",
        "print(f\"   Device: {DEVICE_SMOKE}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "smoke_cmd = [\n",
        "    sys.executable, \"training/run_train_jax.py\",  # Use launcher for XLA env vars\n",
        "    \"--config\", CONFIG_PATH,\n",
        "    \"--smoke_config\", SMOKE_CONFIG_PATH,  # Tiny model for smoke\n",
        "    \"--output\", SMOKE_OUTPUT_DIR,\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--device\", DEVICE_SMOKE,  # M37: Use DEVICE_SMOKE for smoke tests\n",
        "    \"--smoke_steps\", str(SMOKE_STEPS),\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(smoke_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(smoke_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Smoke run completed successfully!\")\n",
        "    print(\"   Pipeline validated. Ready for full TPU training.\")\n",
        "    print(\"   ‚ö†Ô∏è  Ensure you have TPU v3-8 selected for the next cell!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Smoke run failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4b. Full TPU Training Run (M38)\n",
        "\n",
        "Run this cell for the complete training with Gemma 2B on TPU.\n",
        "\n",
        "**‚ö†Ô∏è REQUIRES TPU v3-8 or v5e-8:** \n",
        "- Gemma 2B does NOT fit on Kaggle T4 GPU (verified in M36)\n",
        "- The training script will **exit with error** if you try to run Gemma on GPU\n",
        "- Go to Settings ‚Üí Accelerator ‚Üí TPU v3-8 **before** running this cell\n",
        "\n",
        "**M38 Changes:**\n",
        "- Uses `%run` instead of subprocess to avoid TPU VFIO device conflicts\n",
        "- Config reduced to seq_len=128, batch=1, grad_accum=8 to avoid HBM OOM\n",
        "- Uses bfloat16 (native TPU support, saves ~50% memory)\n",
        "\n",
        "**Time budget:** ~30-60 min for 200 steps on TPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FULL TPU TRAINING RUN (M38)\n",
        "# This runs the complete training pipeline with the configured parameters\n",
        "#\n",
        "# M38: Uses runpy.run_path() to avoid TPU VFIO device conflicts.\n",
        "# After JAX initializes TPU, subprocess.run() can cause \"device busy\" errors.\n",
        "# runpy executes in the same Python process, avoiding this issue.\n",
        "#\n",
        "# ‚ö†Ô∏è  REQUIRES TPU: This will exit with error if you try to run on GPU.\n",
        "\n",
        "import jax\n",
        "import sys\n",
        "import runpy\n",
        "\n",
        "backend = jax.default_backend()\n",
        "if backend != \"tpu\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"‚ùå ERROR: TPU NOT DETECTED\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"   Current backend: {backend}\")\n",
        "    print(f\"   Expected: tpu\")\n",
        "    print(\"\")\n",
        "    print(\"   To fix:\")\n",
        "    print(\"   1. Go to Settings (gear icon in top right)\")\n",
        "    print(\"   2. Scroll to 'Accelerator'\")\n",
        "    print(\"   3. Select 'TPU v3-8' or 'TPU v5e-8'\")\n",
        "    print(\"   4. Click 'Save' and wait for session restart\")\n",
        "    print(\"   5. Re-run this notebook from Cell 2\")\n",
        "    print(\"=\" * 60)\n",
        "    raise RuntimeError(\"TPU required for full training. See instructions above.\")\n",
        "\n",
        "print(\"üöÄ Starting Full TPU Training Run (M38)...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Config:    {CONFIG_PATH}\")\n",
        "print(f\"Model:     {MODEL_NAME}\")\n",
        "print(f\"Dataset:   {DATASET}\")\n",
        "print(f\"Steps:     {MAX_STEPS} (from config)\")\n",
        "print(f\"Device:    {DEVICE_FULL} (TPU)\")\n",
        "print(f\"Output:    {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# M38: Use runpy.run_path() instead of %run to properly catch failures\n",
        "# This runs the training script in the same Python process\n",
        "train_args = [\n",
        "    \"training/run_train_jax.py\",\n",
        "    \"--config\", CONFIG_PATH,\n",
        "    \"--output\", OUTPUT_DIR,\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--device\", DEVICE_FULL,\n",
        "    \"--save_every_steps\", \"50\"\n",
        "]\n",
        "print(f\"Command: python {' '.join(train_args)}\\n\")\n",
        "\n",
        "# Save original sys.argv and replace with training args\n",
        "original_argv = sys.argv\n",
        "sys.argv = train_args\n",
        "training_success = False\n",
        "\n",
        "try:\n",
        "    runpy.run_path(\"training/run_train_jax.py\", run_name=\"__main__\")\n",
        "    training_success = True\n",
        "except SystemExit as e:\n",
        "    if e.code == 0:\n",
        "        training_success = True\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"‚ùå Training FAILED with exit code {e.code}\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"   Check the error messages above for details.\")\n",
        "        print(\"   Common issues:\")\n",
        "        print(\"   ‚Ä¢ HBM OOM: reduce max_length or batch_size in config\")\n",
        "        print(\"   ‚Ä¢ TPU not available: check accelerator settings\")\n",
        "        print(\"=\" * 60)\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"‚ùå Training FAILED with exception: {type(e).__name__}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"   {e}\")\n",
        "finally:\n",
        "    sys.argv = original_argv\n",
        "\n",
        "if training_success:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ TPU Training completed successfully!\")\n",
        "    print(\"   Evidence artifacts saved to:\", OUTPUT_DIR)\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Training did not complete. See errors above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Predictions\n",
        "\n",
        "Generate predictions on the evaluation set using the trained checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the evaluation set\n",
        "\n",
        "predictions_file = f\"{OUTPUT_DIR}/predictions.jsonl\"\n",
        "\n",
        "print(\"üìä Generating predictions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_cmd = [\n",
        "    sys.executable, \"training/eval_generate.py\",\n",
        "    \"--checkpoint\", OUTPUT_DIR,\n",
        "    \"--eval_set\", EVAL_SET,\n",
        "    \"--output\", predictions_file,\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(eval_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(eval_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úÖ Predictions generated successfully!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Prediction generation failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate & Score\n",
        "\n",
        "Score predictions using the evaluation script with scorecard breakdown.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score predictions using the evaluation script\n",
        "\n",
        "print(\"üìà Scoring predictions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "score_cmd = [\n",
        "    sys.executable, \"training/eval_report.py\",\n",
        "    \"--predictions\", predictions_file,\n",
        "    \"--eval_set\", EVAL_SET,\n",
        "]\n",
        "\n",
        "print(f\"Command: {' '.join(score_cmd)}\\n\")\n",
        "\n",
        "result = subprocess.run(score_cmd, capture_output=False)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úÖ Evaluation complete!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Evaluation failed with exit code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Submission Summary\n",
        "\n",
        "Displays final results with a **RESULT SUMMARY** block for easy evidence capture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final submission summary with RESULT SUMMARY block for evidence capture\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"         SUBMISSION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "output_path = Path(OUTPUT_DIR)\n",
        "\n",
        "# Model info\n",
        "print(f\"\\nüì¶ Model ID: {MODEL_NAME}\")\n",
        "print(f\"üìÅ Dataset:  {DATASET}\")\n",
        "print(f\"üìã Eval Set: {EVAL_SET}\")\n",
        "print(f\"üî¢ Steps:    {MAX_STEPS}\")\n",
        "print(f\"üé≤ Seed:     {SEED}\")\n",
        "\n",
        "# Training metrics\n",
        "metrics_file = output_path / \"metrics.jsonl\"\n",
        "final_loss = None\n",
        "if metrics_file.exists():\n",
        "    print(f\"\\nüìä Training Metrics (last 5 steps):\")\n",
        "    with open(metrics_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-5:]:\n",
        "            metric = json.loads(line)\n",
        "            step = metric.get('step', '?')\n",
        "            loss = metric.get('loss', '?')\n",
        "            if isinstance(loss, float):\n",
        "                print(f\"   Step {step}: loss={loss:.4f}\")\n",
        "                final_loss = loss\n",
        "            else:\n",
        "                print(f\"   Step {step}: loss={loss}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Metrics file not found at {metrics_file}\")\n",
        "\n",
        "# Eval score and scorecard\n",
        "eval_results_file = output_path / \"eval_results.json\"\n",
        "primary_score = None\n",
        "scorecard_info = {}\n",
        "if eval_results_file.exists():\n",
        "    with open(eval_results_file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "        primary_score = results.get('primary_score', results.get('answer_correctness'))\n",
        "        scorecard_info = results.get('scorecard', {})\n",
        "        if isinstance(primary_score, float):\n",
        "            print(f\"\\nüéØ Primary Score: {primary_score:.4f} ({primary_score * 100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"\\nüéØ Primary Score: {primary_score}\")\n",
        "        \n",
        "        # M36: Display scorecard if available\n",
        "        if scorecard_info:\n",
        "            n_items = scorecard_info.get('n_items', '?')\n",
        "            n_scored = scorecard_info.get('n_scored', '?')\n",
        "            print(f\"üìä Scorecard: {n_scored}/{n_items} items scored\")\n",
        "            section_scores = scorecard_info.get('section_scores', {})\n",
        "            for section, score in section_scores.items():\n",
        "                if score is not None:\n",
        "                    print(f\"   {section}: {score:.2f}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Eval results not found (run evaluation cell first)\")\n",
        "\n",
        "# Artifact paths\n",
        "print(f\"\\nüìÇ Artifact Paths:\")\n",
        "if output_path.exists():\n",
        "    checkpoints = list(output_path.glob(\"checkpoint*\"))\n",
        "    for ckpt in checkpoints:\n",
        "        print(f\"   {ckpt}\")\n",
        "if metrics_file.exists():\n",
        "    print(f\"   {metrics_file}\")\n",
        "preds_path = Path(predictions_file)\n",
        "if preds_path.exists():\n",
        "    print(f\"   {preds_path}\")\n",
        "\n",
        "# M36: Print RESULT SUMMARY block for evidence capture\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"         RESULT SUMMARY (copy to evidence files)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"model_id: {MODEL_NAME}\")\n",
        "print(f\"dataset: {DATASET}\")\n",
        "print(f\"eval_set: {EVAL_SET}\")\n",
        "print(f\"primary_score: {primary_score}\")\n",
        "print(f\"final_loss: {final_loss}\")\n",
        "print(f\"n_items: {scorecard_info.get('n_items', 'N/A')}\")\n",
        "print(f\"n_scored: {scorecard_info.get('n_scored', 'N/A')}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n‚úÖ Submission package ready!\")\n",
        "print(\"   See docs/submission_checklist.md for next steps.\")\n",
        "print(\"   Evidence folder: submission_runs/m37_v1/\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
