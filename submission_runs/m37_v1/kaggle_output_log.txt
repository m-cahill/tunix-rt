# M38 TPU Training Log
# ====================
#
# This file should contain the full output from the Kaggle TPU training run.
# Copy/paste the output from cells 4-18 of the kaggle_submission.ipynb notebook.
#
# M38 Changes:
# - Uses %run instead of subprocess (avoids TPU VFIO conflicts)
# - seq_len=128 (not 512) to avoid HBM OOM from Gemma's 256K vocab
# - batch=1 with grad_accum=8 (effective batch=8)
# - bfloat16 (native TPU support, saves ~50% HBM)
#
# Required sections:
# 1. JAX device detection (should show TPU, 8 cores)
# 2. Model loading logs (bfloat16 should be shown)
# 3. Training loss progression (every 10 steps)
# 4. Checkpoint saves (steps 50, 100, 150, 200)
# 5. Final RESULT SUMMARY block
#
# Example expected output:
#
# JAX version: 0.4.x
# Available devices: [TpuDevice(id=0, ...), ...]
# Default backend: tpu
# âœ… Running on TPU. Full Gemma training is supported.
#
# ðŸš€ Starting Full TPU Training Run (M38)...
# ============================================================
# Config:    training/configs/submission_tpu.yaml
# Model:     google/gemma-2b
# Dataset:   dev-reasoning-v2
# Steps:     200 (from config)
# Output:    ./output/tpu_run
# ============================================================
#
# ðŸ”§ TPU mode: Using bfloat16 (native TPU support, saves ~50% HBM)
#    Device request: TPU (8 cores available)
#    Platform: TPU
#    Device Count: 8
#
# [Training logs with loss values at steps 10, 20, ..., 200]
#
# ============================================================
#          RESULT SUMMARY (copy to evidence files)
# ============================================================
# model_id: google/gemma-2b
# dataset: dev-reasoning-v2
# eval_set: training/evalsets/eval_v2.jsonl
# primary_score: X.XXXX
# final_loss: X.XXXX
# n_items: 100
# n_scored: XX
# ============================================================
#
# --- PASTE ACTUAL OUTPUT BELOW THIS LINE ---


