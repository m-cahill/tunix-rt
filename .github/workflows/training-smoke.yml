name: Training Smoke Test

# This workflow is OPTIONAL and NON-BLOCKING
# It validates the training workflow with full dependencies (JAX, Flax)
# Runs on: manual dispatch or nightly schedule

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily

permissions:
  contents: read

jobs:
  training-smoke:
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking: failures don't block main CI
    defaults:
      run:
        working-directory: backend

    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.3.0
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'backend/pyproject.toml'

      - name: Install dependencies with training extra
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e ".[dev,training]"

      - name: Verify training dependencies
        run: |
          python -c "import jax; print(f'JAX version: {jax.__version__}')"
          python -c "import flax; print(f'Flax version: {flax.__version__}')"

      - name: Create test dataset
        run: |
          # Run a quick test to create a small dataset
          pytest tests/test_datasets.py::TestDatasetBuildEndpoint::test_build_dataset_latest_strategy -v

      - name: Find generated dataset
        id: find_dataset
        run: |
          # Find the most recently created dataset
          DATASET_DIR=$(ls -td backend/datasets/*/ 2>/dev/null | head -1 || echo "")
          if [ -z "$DATASET_DIR" ]; then
            echo "No dataset found, creating one via API simulation"
            # Create a minimal test dataset programmatically
            python -c "
          import json
          from pathlib import Path
          from datetime import datetime, timezone
          import uuid

          # Create datasets directory
          datasets_dir = Path('datasets')
          datasets_dir.mkdir(exist_ok=True)

          # Create a test dataset
          dataset_dir = datasets_dir / 'test_smoke-v1'
          dataset_dir.mkdir(exist_ok=True)

          # Create manifest
          manifest = {
              'dataset_key': 'test_smoke-v1',
              'build_id': str(uuid.uuid4()),
              'dataset_name': 'test_smoke',
              'dataset_version': 'v1',
              'dataset_schema_version': '1.0',
              'created_at': datetime.now(timezone.utc).isoformat(),
              'filters': {},
              'selection_strategy': 'latest',
              'seed': None,
              'trace_ids': [],
              'trace_count': 0,
              'stats': {}
          }

          with open(dataset_dir / 'manifest.json', 'w') as f:
              json.dump(manifest, f, indent=2)

          # Create sample JSONL
          with open(dataset_dir / 'export.jsonl', 'w') as f:
              for i in range(5):
                  record = {
                      'id': str(uuid.uuid4()),
                      'prompts': f'<start_of_turn>user\nTest prompt {i}<end_of_turn>\n<start_of_turn>model\nReasoning:\n1. Step {i}\nAnswer: {i}<end_of_turn>',
                      'final_answer': str(i),
                      'metadata': {'format': 'tunix_sft', 'index': i}
                  }
                  f.write(json.dumps(record) + '\n')

          print(f'Created test dataset at {dataset_dir}')
            "
            DATASET_DIR="backend/datasets/test_smoke-v1/"
          fi
          echo "dataset_dir=$DATASET_DIR" >> $GITHUB_OUTPUT

      - name: Run smoke test
        run: |
          # Run smoke test on the dataset
          JSONL_FILE="${{ steps.find_dataset.outputs.dataset_dir }}/export.jsonl"

          # If export.jsonl doesn't exist, create it from manifest
          if [ ! -f "$JSONL_FILE" ]; then
            echo "Creating export.jsonl for smoke test..."
            # For now, just ensure we have a valid JSONL file
            python -c "
          import json
          import uuid

          # Create minimal valid JSONL
          with open('$JSONL_FILE', 'w') as f:
              for i in range(5):
                  record = {
                      'id': str(uuid.uuid4()),
                      'prompts': f'<start_of_turn>user\nTest {i}<end_of_turn>\n<start_of_turn>model\nAnswer: {i}<end_of_turn>',
                      'final_answer': str(i),
                      'metadata': {'format': 'tunix_sft'}
                  }
                  f.write(json.dumps(record) + '\n')
            "
          fi

          # Run the smoke test
          python training/sft_smoke.py "$JSONL_FILE" --samples 5

      - name: Summary
        if: success()
        run: |
          echo "✅ Training smoke test PASSED"
          echo "Dataset pipeline validated with JAX/Flax dependencies"

      - name: Failure notice
        if: failure()
        run: |
          echo "⚠️  Training smoke test failed (non-blocking)"
          echo "This is an optional workflow - it does not block main CI"
