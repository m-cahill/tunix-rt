name: CI

on:
  pull_request:
  push:
    branches: [main]

permissions:
  contents: read

jobs:
  # Path filter job - always runs
  # NOTE: paths-filter requires concrete commit SHAs, not symbolic refs like HEAD.
  # - base/ref are resolved from event payloads to ensure deterministic behavior
  # - For PRs: use pull_request.base.sha and pull_request.head.sha
  # - For pushes: use github.event.before and github.sha
  # - fetch-depth: 0 ensures full history is available for diff computation
  # This prevents CI nondeterminism when HEAD doesn't exist or points incorrectly.
  changes:
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      e2e: ${{ steps.filter.outputs.e2e }}
      workflow: ${{ steps.filter.outputs.workflow }}
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0

      # Sanity check: Ensure base and ref SHAs are resolved correctly
      - name: Validate paths-filter inputs
        run: |
          BASE="${{ github.event_name == 'pull_request' && github.event.pull_request.base.sha || github.event.before }}"
          REF="${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}"
          echo "Base SHA: $BASE"
          echo "Ref SHA: $REF"
          if [ -z "$BASE" ] || [ -z "$REF" ]; then
            echo "::error::paths-filter base or ref SHA is empty — workflow misconfigured"
            exit 1
          fi

      - uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
        id: filter
        with:
          token: ''
          base: ${{ github.event_name == 'pull_request' && github.event.pull_request.base.sha || github.event.before }}
          ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
          filters: |
            backend:
              - 'backend/**'
            frontend:
              - 'frontend/**'
            e2e:
              - 'e2e/**'
            workflow:
              - '.github/workflows/**'

  # Backend job - runs if backend or workflow changed
  backend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.workflow == 'true'
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.3.0
        with:
          python-version: ${{ matrix.python-version }}

      - uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "backend/uv.lock"

      - name: Install dependencies
        run: uv sync --locked --extra dev

      - name: Ruff check
        run: uv run ruff check .

      - name: Ruff format check
        run: uv run ruff format --check .

      - name: mypy type check
        run: uv run mypy tunix_rt_backend

      - name: Validate dataset schemas
        run: |
          # Validate dataset schemas can be imported
          uv run python -c "from tunix_rt_backend.schemas.dataset import DatasetManifest, DatasetBuildRequest; print('✅ Dataset schemas valid')"
          # Validate renderer works
          uv run python -c "from tunix_rt_backend.training.renderers import render_tunix_sft_prompt; trace={'prompt':'test','trace_steps':['step'],'final_answer':'answer'}; result=render_tunix_sft_prompt(trace); assert '<start_of_turn>' in result; print('✅ Renderer works')"

      - name: pytest with coverage
        run: uv run pytest -q --cov=tunix_rt_backend --cov-branch --cov-config=.coveragerc --cov-report=term --cov-report=xml --cov-report=json:coverage.json

      - name: Enforce coverage gates (line ≥80%, branch ≥68%)
        run: uv run python tools/coverage_gate.py

      - name: Upload coverage
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        with:
          name: backend-coverage-py${{ matrix.python-version }}
          path: backend/coverage.xml

      - name: Migration smoke test
        run: |
          export DATABASE_URL="sqlite+aiosqlite:///./test_migrations.db"
          uv run alembic upgrade head
        env:
          DATABASE_URL: "sqlite+aiosqlite:///./test_migrations.db"

  # Frontend job - runs if frontend or workflow changed
  frontend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.frontend == 'true' || needs.changes.outputs.workflow == 'true'
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-node@39370e3970a6d050c480ffad4ff0ed4d3fdee5af # v4.1.0
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Install dependencies
        run: npm ci

      - name: Run tests with coverage
        run: npm run test -- --coverage

      - name: Upload coverage
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        with:
          name: frontend-coverage
          path: frontend/coverage/
          retention-days: 30

      - name: Build
        run: npm run build

  # Security: pip-audit (warn-only)
  security-backend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.workflow == 'true'
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.3.0
        with:
          python-version: '3.11'

      - uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "backend/uv.lock"

      - name: Install dependencies
        run: uv sync --locked --extra dev

      - name: Run pip-audit
        run: |
          uv run pip install pip-audit
          uv run pip-audit --desc --format json --output pip-audit-report.json || true
          uv run pip-audit --desc || true
        continue-on-error: true

      - name: Upload pip-audit report
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: pip-audit-report
          path: backend/pip-audit-report.json
          retention-days: 30

      # M11 Phase 1: SBOM generation re-enabled using cyclonedx-py CLI
      - name: Install SBOM tool
        run: uv run pip install cyclonedx-bom

      - name: Generate SBOM
        # Generate SBOM from current environment (packages already installed)
        run: uv run cyclonedx-py environment --output-format JSON --output-file sbom.json
        continue-on-error: true

      - name: Upload SBOM
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: backend-sbom
          path: backend/sbom.json
          retention-days: 90

  # Security: npm audit (warn-only)
  security-frontend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.frontend == 'true' || needs.changes.outputs.workflow == 'true'
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-node@39370e3970a6d050c480ffad4ff0ed4d3fdee5af # v4.1.0
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Install dependencies
        run: npm ci

      - name: Run npm audit
        run: |
          npm audit --json > npm-audit-report.json || true
          npm audit || true
        continue-on-error: true

      - name: Upload npm audit report
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: npm-audit-report
          path: frontend/npm-audit-report.json
          retention-days: 30

  # Security: Secret scanning (runs on push to main only to avoid PR API calls)
  security-secrets:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0

      - name: Run Gitleaks
        uses: gitleaks/gitleaks-action@ff98106e4c7b2bc287b24eaf42907196329070c7 # v2.3.9
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # E2E job - runs if backend, frontend, e2e, or workflow changed
  # M4: Added postgres service, migrations, and smoke checks for deterministic E2E
  e2e:
    runs-on: ubuntu-latest
    needs: changes
    if: |
      needs.changes.outputs.backend == 'true' ||
      needs.changes.outputs.frontend == 'true' ||
      needs.changes.outputs.e2e == 'true' ||
      needs.changes.outputs.workflow == 'true'

    # M4: Add Postgres service container with healthcheck
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    defaults:
      run:
        working-directory: e2e

    env:
      REDIAI_MODE: mock
      # M4: Explicit DATABASE_URL for clarity (matches service container)
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/postgres

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.3.0
        with:
          python-version: '3.11'

      - uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "backend/uv.lock"

      - name: Install backend dependencies
        run: |
          cd ../backend
          uv sync --locked --extra dev
          # Add venv to PATH so Playwright picks up uvicorn/python
          echo "$PWD/.venv/bin" >> $GITHUB_PATH

      # M4: Run migrations before starting servers
      - name: Run database migrations
        run: cd ../backend && uv run alembic upgrade head
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/postgres

      - uses: actions/setup-node@39370e3970a6d050c480ffad4ff0ed4d3fdee5af # v4.1.0
        with:
          node-version: '18'

      - name: Install frontend dependencies
        run: cd ../frontend && npm ci

      - name: Install E2E dependencies
        run: npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('e2e/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install Playwright browsers
        run: npx playwright install chromium --with-deps

      # M4: Playwright webServer will start backend + frontend and wait for health URLs
      # No explicit smoke check needed - webServer.url provides built-in waiting
      - name: Run Playwright tests
        run: npx playwright test

      - uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: playwright-report
          path: e2e/playwright-report/
          retention-days: 7
