name: CI

on:
  pull_request:
  push:
    branches: [main]

permissions:
  contents: read

jobs:
  # Path filter job - always runs
  # NOTE: paths-filter requires concrete commit SHAs, not symbolic refs like HEAD.
  # - base/ref are resolved from event payloads to ensure deterministic behavior
  # - For PRs: use pull_request.base.sha and pull_request.head.sha
  # - For pushes: use github.event.before and github.sha
  # - fetch-depth: 0 ensures full history is available for diff computation
  # This prevents CI nondeterminism when HEAD doesn't exist or points incorrectly.
  changes:
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      e2e: ${{ steps.filter.outputs.e2e }}
      workflow: ${{ steps.filter.outputs.workflow }}
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0

      # Sanity check: Ensure base and ref SHAs are resolved correctly
      - name: Validate paths-filter inputs
        run: |
          BASE="${{ github.event_name == 'pull_request' && github.event.pull_request.base.sha || github.event.before }}"
          REF="${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}"
          echo "Base SHA: $BASE"
          echo "Ref SHA: $REF"
          if [ -z "$BASE" ] || [ -z "$REF" ]; then
            echo "::error::paths-filter base or ref SHA is empty — workflow misconfigured"
            exit 1
          fi

      - uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
        id: filter
        with:
          token: ''
          base: ${{ github.event_name == 'pull_request' && github.event.pull_request.base.sha || github.event.before }}
          ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
          filters: |
            backend:
              - 'backend/**'
            frontend:
              - 'frontend/**'
            e2e:
              - 'e2e/**'
            workflow:
              - '.github/workflows/**'

  # Backend job - runs if backend or workflow changed
  backend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.workflow == 'true'
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.3.0
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: 'backend/pyproject.toml'

      - name: Install dependencies
        run: python -m pip install -e ".[dev]"

      - name: Ruff check
        run: ruff check .

      - name: Ruff format check
        run: ruff format --check .

      - name: mypy type check
        run: mypy tunix_rt_backend

      - name: Validate dataset schemas
        run: |
          # Validate dataset schemas can be imported
          python -c "from tunix_rt_backend.schemas.dataset import DatasetManifest, DatasetBuildRequest; print('✅ Dataset schemas valid')"
          # Validate renderer works
          python -c "from tunix_rt_backend.training.renderers import render_tunix_sft_prompt; trace={'prompt':'test','trace_steps':['step'],'final_answer':'answer'}; result=render_tunix_sft_prompt(trace); assert '<start_of_turn>' in result; print('✅ Renderer works')"

      - name: pytest with coverage
        run: pytest -q --cov=tunix_rt_backend --cov-branch --cov-config=.coveragerc --cov-report=term --cov-report=xml --cov-report=json:coverage.json

      - name: Enforce coverage gates (line ≥80%, branch ≥68%)
        run: python tools/coverage_gate.py

      - name: Upload coverage
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        with:
          name: backend-coverage-py${{ matrix.python-version }}
          path: backend/coverage.xml

      - name: Migration smoke test
        run: |
          export DATABASE_URL="sqlite+aiosqlite:///./test_migrations.db"
          alembic upgrade head
        env:
          DATABASE_URL: "sqlite+aiosqlite:///./test_migrations.db"

  # Frontend job - runs if frontend or workflow changed
  frontend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.frontend == 'true' || needs.changes.outputs.workflow == 'true'
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-node@39370e3970a6d050c480ffad4ff0ed4d3fdee5af # v4.1.0
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Install dependencies
        run: npm ci

      - name: Run tests with coverage
        run: npm run test -- --coverage

      - name: Upload coverage
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        with:
          name: frontend-coverage
          path: frontend/coverage/
          retention-days: 30

      - name: Build
        run: npm run build

  # Security: pip-audit (warn-only)
  security-backend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.workflow == 'true'
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.3.0
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'backend/pyproject.toml'

      - name: Install dependencies
        run: python -m pip install -e ".[dev]"

      - name: Cache security tools
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: ~/.cache/pip
          key: security-tools-${{ runner.os }}-${{ hashFiles('backend/pyproject.toml') }}
          restore-keys: |
            security-tools-${{ runner.os }}-

      - name: Run pip-audit
        run: |
          pip install pip-audit
          pip-audit --desc --format json --output pip-audit-report.json || true
          pip-audit --desc || true
        continue-on-error: true

      - name: Upload pip-audit report
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: pip-audit-report
          path: backend/pip-audit-report.json
          retention-days: 30

      # M11 Phase 1: SBOM generation re-enabled using cyclonedx-py CLI
      - name: Install SBOM tool
        run: pip install cyclonedx-bom

      - name: Generate SBOM
        run: |
          cd ../backend
          cyclonedx-py requirements -i pyproject.toml -o sbom.json --format json
        continue-on-error: true

      - name: Upload SBOM
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: backend-sbom
          path: backend/sbom.json
          retention-days: 90

  # Security: npm audit (warn-only)
  security-frontend:
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.frontend == 'true' || needs.changes.outputs.workflow == 'true'
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-node@39370e3970a6d050c480ffad4ff0ed4d3fdee5af # v4.1.0
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Install dependencies
        run: npm ci

      - name: Run npm audit
        run: |
          npm audit --json > npm-audit-report.json || true
          npm audit || true
        continue-on-error: true

      - name: Upload npm audit report
        uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: npm-audit-report
          path: frontend/npm-audit-report.json
          retention-days: 30

  # Security: Secret scanning (runs on push to main only to avoid PR API calls)
  security-secrets:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0

      - name: Run Gitleaks
        uses: gitleaks/gitleaks-action@ff98106e4c7b2bc287b24eaf42907196329070c7 # v2.3.9
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # E2E job - runs if backend, frontend, e2e, or workflow changed
  # M4: Added postgres service, migrations, and smoke checks for deterministic E2E
  e2e:
    runs-on: ubuntu-latest
    needs: changes
    if: |
      needs.changes.outputs.backend == 'true' ||
      needs.changes.outputs.frontend == 'true' ||
      needs.changes.outputs.e2e == 'true' ||
      needs.changes.outputs.workflow == 'true'

    # M4: Add Postgres service container with healthcheck
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    defaults:
      run:
        working-directory: e2e

    env:
      REDIAI_MODE: mock
      # M4: Explicit DATABASE_URL for clarity (matches service container)
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/postgres

    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.3.0
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'backend/pyproject.toml'

      - name: Install backend dependencies
        run: cd ../backend && python -m pip install -e ".[dev]"

      # M4: Run migrations before starting servers
      - name: Run database migrations
        run: cd ../backend && alembic upgrade head
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/postgres

      - uses: actions/setup-node@39370e3970a6d050c480ffad4ff0ed4d3fdee5af # v4.1.0
        with:
          node-version: '18'

      - name: Install frontend dependencies
        run: cd ../frontend && npm ci

      - name: Install E2E dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install chromium --with-deps

      # M4: Playwright webServer will start backend + frontend and wait for health URLs
      # No explicit smoke check needed - webServer.url provides built-in waiting
      - name: Run Playwright tests
        run: npx playwright test

      - uses: actions/upload-artifact@6f51ac03b9356f520e9adb1b1b7802705f340c2b # v4.5.0
        if: always()
        with:
          name: playwright-report
          path: e2e/playwright-report/
          retention-days: 7
